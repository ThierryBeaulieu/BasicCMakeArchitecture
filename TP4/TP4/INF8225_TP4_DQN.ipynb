{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThierryBeaulieu/BasicCMakeArchitecture/blob/master/TP4/TP4/INF8225_TP4_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553UZ1iuVOV7"
      },
      "source": [
        "# TP4, INF8225 2025, Projet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjWK-m8VOV8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XSGJpgj7VOV8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install numpy\n",
        "%pip install swig\n",
        "%pip install box2d\n",
        "%pip install pygame\n",
        "%pip install gymnasium\n",
        "%pip install \"gymnasium[box2d]\"\n",
        "%pip install matplotlib\n",
        "%pip install wandb\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fcRnqGzMVOV8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import matplotlib.animation as animation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "import os\n",
        "import wandb\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "from collections import namedtuple, deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28fvb3eqVOV8"
      },
      "source": [
        "### Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p2wOLXAUVOV8",
        "outputId": "8867c865-36b6-4ef6-c652-c0249d8df724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "12.1\n",
            "NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(torch.version.cuda)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\tprint(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCvxsgx4VOV8"
      },
      "source": [
        "## Data Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "x2GvSSPqVOV8",
        "outputId": "c8d847fb-97bc-4aa1-b411-c2083a3ef21b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space:  Box(0, 255, (96, 96, 3), uint8)\n",
            "Action space:  Discrete(5)\n"
          ]
        }
      ],
      "source": [
        "# Inspired by : https://github.com/pangyyen/carRacing-DeepRL/blob/main/ppo/ppo.ipynb\n",
        "\n",
        "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", domain_randomize=False, continuous=False)\n",
        "print(\"Observation space: \", env.observation_space) # (low, high, shape, dtype)\n",
        "print(\"Action space: \", env.action_space)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "observation, info = env.reset(seed=SEED)\n",
        "def show_animation():\n",
        "\tshow_animation_frames(env.render())\n",
        "\n",
        "def show_animation_frames(frames):\n",
        "\tfig = plt.figure(figsize=(7, 5))\n",
        "\tplt.axis('off')\n",
        "\tim = plt.imshow(frames[0])\n",
        "\n",
        "\tdef animate(i):\n",
        "\t\tim.set_data(frames[i])\n",
        "\t\treturn im,\n",
        "\n",
        "\tanim = animation.FuncAnimation(fig, animate, frames=len(frames), repeat=False)\n",
        "\tplt.close(fig)\n",
        "\tdisplay(HTML(anim.to_jshtml()))\n",
        "\n",
        "def show_current_frame(env, data):\n",
        "\tframe = env.render()\n",
        "\tfig, _ = plt.subplots()\n",
        "\tr = fig.canvas.get_renderer()\n",
        "\tplt.imshow(frame)\n",
        "\tplt.axis('off')\n",
        "\ttexts = []\n",
        "\tsize_used = 0\n",
        "\tfor i, key in enumerate(data):\n",
        "\t\ttext = plt.text(0, 0, f'{key}: {data[key]}', fontsize=12, color='black', backgroundcolor='white', ha=\"center\")\n",
        "\t\tsize_used += text.get_window_extent(renderer=r).width\n",
        "\t\ttexts.append(text)\n",
        "\tsplit = (700 - size_used) / (len(data) + 1)\n",
        "\tnext_position = split\n",
        "\tfor t in texts:\n",
        "\t\tt.set_position((next_position, 0))\n",
        "\t\tnext_position = next_position + t.get_window_extent(renderer=r).width + split\n",
        "\tclear_output(wait=True)\n",
        "\tplt.show()\n",
        "\n",
        "def skip_zooming(env):\n",
        "\tno_action = 0\n",
        "\tif type(env.action_space) != gym.spaces.Discrete:\n",
        "\t\tno_action = np.zeros((env.action_space.shape[0]))\n",
        "\n",
        "\tfor i in range(50):\n",
        "\t\tobservation, _, terminated, truncated, info = env.step(no_action)\n",
        "\n",
        "\t\tif terminated or truncated:\n",
        "\t\t\tobservation, info = env.reset()\n",
        "\t\t\tbreak\n",
        "\treturn observation, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYfc-8GFVOV9"
      },
      "source": [
        "### Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KSkwQhR8VOV9"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "\tT.ToPILImage(),\n",
        "\tT.Grayscale(num_output_channels=1),\n",
        "\tT.Resize((84, 84)),\n",
        "\tT.ToTensor(),\n",
        "\tT.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XooYEygsVOV9"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALVCgGGoVOV9"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6co9R30VVOV9"
      },
      "source": [
        "#### Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIRvRtvxVOV9"
      },
      "source": [
        "DQN is at its heart Q-Learning using Deep Neural Networks to predict the behavior of its environment and to predict which action is the best.\n",
        "\n",
        "Our goal, when implementing DQN is to maximize the rewards of our policy $\\pi^{*}$ described as followed, where $Q^{*}$ is defined as the optimal action-value function.\n",
        "\n",
        "$$\n",
        "\\pi^{*}(s) = \\underset{a}{\\arg\\max} \\; Q^{*}(s,a)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "The definition of $Q^{*}$ follows the Bellman Optimality Equation:\n",
        "\n",
        "$$\n",
        "Q^{*}(s,a) = \\mathbb{E} \\left[ r + \\gamma \\underset{a'}{\\max} Q^{*}(s', a') \\; | \\; s, a \\right]\n",
        "$$\n",
        "\n",
        "The equation means that the value of an action is dictated by the current reward + the best reward we can get from the best next action. The $\\gamma$ symbol is used only so that we can diminishes the importance of futur action on the long run.\n",
        "\n",
        "Our goal is to maximize the rewards we will have on the long term, which can be defined as:\n",
        "\n",
        "$$\n",
        "G_t = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\gamma^{3} r_{t+3} + ...  \n",
        "$$\n",
        "\n",
        "Based on Bellman's Optimality Equation, we are able to use the following update equation:\n",
        "\n",
        "$$\n",
        "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\underset{a'}{\\max}Q(s', a') - Q(s,a) \\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZunLW1MVOV9"
      },
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Jd0qR4DEVOV9"
      },
      "outputs": [],
      "source": [
        "# Implementation based on : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def append(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, n_actions):\n",
        "    \"\"\"\n",
        "    Q-Network made of a Deep neural network\n",
        "    \"\"\"\n",
        "    super(DQN, self).__init__()\n",
        "    # TODO: Ajust the depth of the model so that we don't need to use 128 each time,\n",
        "    # and evaluate the impact of changing those values\n",
        "    self.net = nn.Sequential(\n",
        "      # Adjusted for RGB input (96x96x3) without resizing\n",
        "      nn.Conv2d(1, 32, kernel_size=8, stride=4),    # Output: 32x23x23\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, kernel_size=4, stride=2),   # Output: 64x10x10\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1),    # Output: 64x8x8\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(64 * 7 * 7, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class DQNAgent():\n",
        "  def __init__(self, env):\n",
        "    \"\"\"\n",
        "    Agent made of DQNs used for learning how to use the sim racer.\n",
        "    \"\"\"\n",
        "    # TODO : make it so that it's possible to verify which hyperparameter was the best\n",
        "    # TODO : we will handle the images as greyscale because we don't need\n",
        "    # to handle the colors, it doesn't add that much information more than greyscale\n",
        "    # TODO : inclure les formules mathÃ©matiques\n",
        "\n",
        "    # Hyperparameters\n",
        "    self.GAMMA = 0.99\n",
        "    self.LR = 3e-4\n",
        "    self.BATCH_SIZE = 64\n",
        "    self.MEMORY_SIZE = 10000\n",
        "    self.EPSILON_START = 1.0\n",
        "    self.EPSILON_END = 0.01\n",
        "    self.EPSILON_DECAY = 1000\n",
        "    self.TARGET_UPDATE_FREQ = 10\n",
        "\n",
        "    # Possible actions\n",
        "    self.discrete_actions = [\n",
        "      0,\t# Do nothing\n",
        "      1,\t# Steer left\n",
        "      2,\t# Steer right\n",
        "      3,\t# Accelerate\n",
        "      4,\t# Brake\n",
        "    ]\n",
        "    n_actions = len(self.discrete_actions)\n",
        "\n",
        "    # Neural Network Declarations Here\n",
        "    self.policy_net = DQN(n_actions).to(device)\n",
        "    self.target_net = DQN(n_actions).to(device)\n",
        "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
        "    self.memory = ReplayMemory(10000)\n",
        "\n",
        "    self.steps_done = 0\n",
        "    self.episode_durations = []\n",
        "\n",
        "  def select_action(self, state):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy strategy\n",
        "\n",
        "    state: contains the rgb image of the car and the racing track (96, 96, 3)\n",
        "    \"\"\"\n",
        "    sample = random.random()\n",
        "    eps_threshold = self.EPSILON_END + (self.EPSILON_START - self.EPSILON_END) * \\\n",
        "      math.exp(-1. * self.steps_done / self.EPSILON_DECAY)\n",
        "    self.steps_done += 1\n",
        "\n",
        "\n",
        "    # print(f\"{eps_threshold}\")\n",
        "    if sample > eps_threshold:\n",
        "      with torch.no_grad():\n",
        "        # Add batch dimension and convert to float\n",
        "        state_tensor = state.unsqueeze(0).float().to(device)\n",
        "        q_values = self.policy_net(state_tensor)\n",
        "        action_idx = q_values.argmax().item()\n",
        "    else:\n",
        "      action_idx = random.randint(0, len(self.discrete_actions)-1)\n",
        "\n",
        "    return action_idx\n",
        "\n",
        "  def preprocess_state(self, state):\n",
        "    # Convert numpy array to tensor and normalize to [0,1]\n",
        "    # Input shape: (96, 96, 3) -> Output shape: (3, 96, 96)\n",
        "    return torch.from_numpy(state).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "  def optimize_model(self):\n",
        "    \"\"\"\n",
        "    Apply the backward propagation to the policy_net and the target_net.\n",
        "    \"\"\"\n",
        "    if len(self.memory) < self.BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    transitions = self.memory.sample(self.BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    states = torch.stack([s for s in batch.state]).to(device)\n",
        "    actions = torch.tensor(batch.action, dtype=torch.long, device=device)\n",
        "    rewards = torch.tensor(batch.reward, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    dones = torch.tensor(batch.done, dtype=torch.bool, device=device).unsqueeze(1)\n",
        "    next_states = torch.stack([s for s in batch.next_state]).to(device)\n",
        "\n",
        "    # Remove flattening steps to keep spatial structure\n",
        "    current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "    next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "    expected_q = rewards + (self.GAMMA * next_q * ~dones)\n",
        "\n",
        "    # loss = nn.MSELoss()(current_q, expected_q)\n",
        "    loss = nn.SmoothL1Loss()(current_q, expected_q)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 10)\n",
        "    self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teWHBJfaVOV9"
      },
      "source": [
        "#### Training Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iljGLxzoVOV9",
        "outputId": "183b9403-121c-42f0-f57f-aae191da9670",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -88.39999999999918\n",
            "Episode 2, Total Reward: -90.59999999999911\n",
            "Episode 3, Total Reward: -27.099999999999874\n",
            "Episode 4, Total Reward: -77.40000000000015\n",
            "Episode 5, Total Reward: -92.79999999999902\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "agent = DQNAgent(env)\n",
        "\n",
        "# Real progress starts at 100\n",
        "n_episode = 700\n",
        "for episode in range(n_episode):\n",
        "\tstate, _ = env.reset(seed=SEED)\n",
        "\tstate, _ = skip_zooming(env)\n",
        "\tstate = transform(state)\n",
        "\ttotal_reward = 0\n",
        "\trewards = []\n",
        "\tdone = False\n",
        "\tt = 0\n",
        "\n",
        "\twhile not done:\n",
        "\t\taction_idx = agent.select_action(state)\n",
        "\t\tnext_state, reward, done, truncated, _ = env.step(action_idx)\n",
        "\t\tdone = done or truncated\n",
        "\n",
        "\t\treward = np.clip(reward, -1, 1)\n",
        "\t\tnext_state = transform(next_state)\n",
        "\n",
        "\t\tagent.memory.append(state, action_idx, next_state, reward, done)\n",
        "\t\trewards.append(reward)\n",
        "\t\tstate = next_state\n",
        "\t\ttotal_reward += reward\n",
        "\n",
        "\t\t# if t % 20 == 0:\n",
        "\t\t# \tshow_current_frame(env, {\"Episode\": episode, \"Timestep\": t})\n",
        "\n",
        "\t\tagent.optimize_model()\n",
        "\n",
        "\t\tt += 1\n",
        "\n",
        "\tif episode % agent.TARGET_UPDATE_FREQ == 0:\n",
        "\t\tagent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "\tprint(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRqpJaHEVOV9"
      },
      "source": [
        "#### Car Racing Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTv24Jv9VOV9"
      },
      "outputs": [],
      "source": [
        "def run_agent_and_collect_frames(agent, env, seed=42):\n",
        "  state, _ = env.reset(seed=seed)\n",
        "  done = False\n",
        "  frames = []\n",
        "\n",
        "  while not done:\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "\n",
        "    preprocessed_state = agent.preprocess_state(state)\n",
        "    action = agent.select_action(preprocessed_state)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    state = next_state\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Function to display frames as an animation using matplotlib\n",
        "def show_animation_frames(frames):\n",
        "  fig = plt.figure(figsize=(7, 5))\n",
        "  plt.axis('off')\n",
        "  im = plt.imshow(frames[0])\n",
        "\n",
        "  def animate(i):\n",
        "    im.set_data(frames[i])\n",
        "    return im,\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50, repeat=False)\n",
        "  plt.close(fig)\n",
        "  display(HTML(anim.to_jshtml()))\n",
        "\n",
        "# Run the episode with the trained agent\n",
        "frames = run_agent_and_collect_frames(agent, env)\n",
        "\n",
        "# Show the animation\n",
        "show_animation_frames(frames)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}